{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqByPCW79XS6IXStrIqljA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trolfe13/Scientific-Computing/blob/main/Lab06_241_Tides.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUnpnU_9PNZn",
        "outputId": "e549afad-bd5e-47aa-e0dd-99ccb772aaa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl==3.1.2 in /usr/local/lib/python3.11/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl==3.1.2) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import periodogram, find_peaks\n",
        "from datetime import datetime\n",
        "import scipy.signal as signal\n",
        "!pip install openpyxl==3.1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 06 - MATH/PHYS 241\n",
        "\n",
        "Processing raw data to later be analyzed is one of the most important steps any scientist performs. There are a variety of methods to filter data and you will want to expose yourself to as many methods as possible so you are prepared for whatever comes your way.\n",
        "\n",
        "Today, we will look at tidal time-series data collected by [NOAA and Woods Hole Oceanographic Institute](https://tidesandcurrents.noaa.gov/stationhome.html?id=8447930).\n",
        "\n",
        "Upload \"WHTides_2005_May_Data.xlsx\" to the notebook as a pandas data frame. You can access todays datafile on CANVAS or my GitHub. Once uploaded, make sure to inspect the file. There are several columns with the following names:\n",
        "\n",
        "* Station_ID\n",
        "* Year\n",
        "* Month\n",
        "* Day\n",
        "* Hour\n",
        "* Minute\n",
        "* Predicted_MLLW\n",
        "* Verified_MLLW\n",
        "\n",
        "MLLW stands for *mean lower low water* which represents the average value of the lowest tide over the recording period. Measurements are made every 6 minutes. This data set is complete for the month of May, 2005 (meaning there are no missing measurements)."
      ],
      "metadata": {
        "id": "LZmZdMA-PdaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decimal Date\n",
        "\n",
        "When we plot time series data, it is very helpful if the x-axis is in a decimal date format instead of regular Julian Calendar day format. This allows us to more easily apply regression models since the slope will be over regular numbers instead of calendar numbers.\n",
        "\n",
        "After you upload the data into a data frame, the first thing we need to do is calculate the decimal date. This is what we will want our x-axis to be when we make a plot of the MLLW vs Time.\n",
        "\n",
        "To calculate the decimal date, you will need figure out a way to \"sum\" the time components into a single value. An easy way to do this is to find what the day number is and divide by total number of days in a year. For example, May 01 is the 121 day of the year (assuming it's not a leap-year), so we just divide 121 by 365 to get the decimal date.\n",
        "\n",
        "This data set include Hours and Mintues. To find the fraction of a day including hours and minutes, we just need to take the hour divided by 24 and add the minute divided by (24 * 60). So, 15:06 on May 01, 2025 would be (6/(24*60) + 15/24 + 121)/365 + 2025 = 2025.3332306"
      ],
      "metadata": {
        "id": "GfEfnOwMn79U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "\n",
        "Calculate the Decimal Date for each data point, then plot Verified_MLLW vs Decimal Date.\n"
      ],
      "metadata": {
        "id": "oWoZd3jveG3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload CSV file\n",
        "df = pd.read_excel('WHTides_2005_May_Data.xlsx')"
      ],
      "metadata": {
        "id": "Jx-vSbl-P-0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smoothing Windows\n",
        "\n",
        "When data is particularly noisy, it is sometimes necessary to smooth the data by applying a [smoothing window](https://en.wikipedia.org/wiki/Window_function). A smoothing window defines a set width called the window, then performs a function over data within the window and returns a single value.\n",
        "\n",
        "There are many different types of smoothing windows to choose from, but two common ones are the [Boxcar Window](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.boxcar.html) and [Hamming Window](https://en.wikipedia.org/wiki/Window_function#Hann_and_Hamming_windows).\n",
        "\n",
        "\n",
        "The Boxcar window simply takes the average of all values within the window while the Hamming Window applies a cosine function and weights all of the values within the window depending on how far away they they are from the middle of the window before taking the sum.\n",
        "\n",
        "The scipy library has a Boxcar function we may use:\n",
        "\n",
        "`bc_window = scipy.signal.windows.boxcar(window_size)`\n",
        "\n",
        "Where `window_size` is the length of the smoothing window.\n",
        "\n",
        "The numpy library has a hamming function we may use:\n",
        "\n",
        "`h_window = np.hamming(window_size)`\n",
        "\n",
        "Lastly, we need to normalize the windows to ensure the weights assigned by the window sum to 1. This prevents the smoothing operation from changing the overall magnitude of the data. To do this, we can use:\n",
        "\n",
        "`bc_window /= bc_window.sum()` \\\\\n",
        "`h_window /= h_window.sum()`\n",
        "\n",
        "Once we have the windows, the last step to to apply the smoothing window to the data. To do this, we have to [convolve](https://en.wikipedia.org/wiki/Convolution) the two arrays. Numpy again has a function we may use:\n",
        "\n",
        "`smoothed_array = np.convolve(a , v, mode='same')`\n",
        "\n",
        "Where `a` is the smoothing window and `v` is the data to be smoothed. Due to the way the smoothing works, the output will have a length $N+m-1$ where $N$ is the length of the unsmoothed data and $m$ is the length of the smoothing window. By including `mode='same'`, we tell python to exclude the added data points from the convolution.\n"
      ],
      "metadata": {
        "id": "wR1TswKSnubb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2\n",
        "\n",
        "Using a smoothing window of 24 hours, smooth the Verified_MLLW data by apply a Boxcar and Hamming smoothing window and convolving.\n",
        "\n",
        "Make a plot of the smoothed data (line plot '-') over the unsmoothed data (point plots '.'). Change the transparency of the unsmoothed data to `alpha = 0.25` so it is somewhat faded.\n",
        "\n",
        "Is there a significant visual difference between the two methods? What happens if you zoom in on the x-axis? Does one method \"better\" preserve peaks than the other?\n",
        "\n",
        "Change the smoothing window to 12, 4, and 1 hours and visually evalute the output. Discuss."
      ],
      "metadata": {
        "id": "qB87QBEpsS2J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjKfLqWdIddq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Periodograms\n",
        "\n",
        "In class, we discussed the idea of Power Spectral Density and made plots to visually display the information. A [periodogram](https://en.wikipedia.org/wiki/Periodogram) is an estimate of the power spectral density and is often used to more quickly evaluate the power associated with specific frequencies present in a signal.\n",
        "\n",
        "Importantly, the [scipy library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html) has a function to help us make a periodogram very easily:\n",
        "\n",
        "`f, Pxx = scipy.signal.periodogram(signal, sr)`\n",
        "\n",
        "Where `signal` is the input signal (in time space) we would like to create the periodogram from and `sr` is the sample rate (in hertz).\n",
        "\n",
        "The output of the function goes into two values, `f` and `Pxx`, which store the frequencies (x-axis) and the power (y-axis)."
      ],
      "metadata": {
        "id": "8NP7vSLUwjQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3\n",
        "\n",
        "Create 3 periodograms, one for the raw (unsmoothed) data, one for the boxcar window smoothed data, and one for the hamming window smoothed data. Make sure you give the sample rate in hertz!\n",
        "\n",
        "Compare the plots. What diffences are noticable? How important is applying a smoothing window to the data?\n",
        "\n",
        "What frequencies have significant peaks associated with them? What time (in hours) do these frequencies correspond to?  \n",
        "\n"
      ],
      "metadata": {
        "id": "tntaQ1i73Ftq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDS2Z-ZzIblU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-whitening Data\n",
        "\n",
        "The analysis we just performed is fine, but if we are trying to be thorough, then we should *pre-whiten* then data. This is usually simply the task of removing any long-term trends and the mean\n",
        "from the data. Long-term trends represent more power at low frequencies (hence a “red” spectrum), and removing them “whitens” the spectrum.\n",
        "\n",
        "As usual, there are many methods to do this and the exact method you want to implement will depend exactly on what type of data you have. For us, we will apply a linear fit the the data and remove the trend. Put another way, we want to calculate the residuals to the linear fit and redo the above analysis."
      ],
      "metadata": {
        "id": "lqjPL6MU7J8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4\n",
        "\n",
        "Apply a linear fit to the raw data and find the residuals of the data to the fit. Using these residuals, apply the boxcar and hamming smoothing windows to smooth the data, then produce the periodograms for the pre-whitened smoothed data."
      ],
      "metadata": {
        "id": "ZJxbgwCv9WDm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQyC-D-5IZHz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}